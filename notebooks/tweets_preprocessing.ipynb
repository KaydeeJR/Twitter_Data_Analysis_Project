{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of Contents:**\n",
    "\n",
    "* [Folder Setup](#folders)\n",
    "* [Reading data from MongoDB](#read)\n",
    "* [Data preprocessing](#preprocessing)\n",
    "* [Topic modelling](#modelling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder setup <a class=\"anchor\" id=\"folders\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(directory_path + \"\\\\utils\")\n",
    "sys.path.append(directory_path + \"\\\\scripts\")\n",
    "sys.path.append(directory_path + \"\\\\notebooks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB connection string is stored in .env folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from MongoDB <a class=\"anchor\" id=\"read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from read_docs import ReadDocs\n",
    "data_access = ReadDocs(os.environ.get('MONGODB_URI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_access.list_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_access.list_collections(\"tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweets_df = data_access.read_tweets_in_collection(\"tweets\",\"global\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "Preprocessing functions to standardize the tweets one word at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base = os.path.dirname(os.getcwd())\n",
    "data_folder = project_base + r\"\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to reading from MongoDB cluster\n",
    "import pandas as pd\n",
    "tweets_df = pd.read_json(data_folder+r\"\\global_twitter_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TweetsPreprocessing\n",
    "tweets_prep = TweetsPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = tweets_prep.preprocess_tweets_df(tweets_df, \"full_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling <a class=\"anchor\" id=\"modelling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modelling import TopicModelling\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = TopicModelling()\n",
    "tweet_mappings = topic_model.make_dictionary(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "topic_model.save_dictionary(tweet_mappings,'global_mappings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = topic_model.create_bow(processed_df, tweet_mappings)\n",
    "topic_model.serialize_bow('global_bow',bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = topic_model.create_lda_model(bow, tweet_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Look into this visualization: maybe try using a single Tweet text instead of an entire dataframe list???\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "display(topic_model.visualize_lda_results(lda, bow, tweet_mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save_lda_objects(lda, 'global_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Try loading saved objects i.e. lda model, mappings, bag of word corpus\n",
    "\n",
    "**TODO**: Pick a topic of interest to investigate (Visualization-guided analysis)\n",
    "\n",
    "**TODO**: Create a wordcloud (Visualize topic 2 with a word cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
