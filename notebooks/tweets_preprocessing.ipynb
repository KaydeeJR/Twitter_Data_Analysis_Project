{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of Contents:**\n",
    "\n",
    "* [Folder Setup](#folders)\n",
    "* [Reading data from MongoDB](#read)\n",
    "* [Data preprocessing](#preprocessing)\n",
    "* [Topic modelling](#modelling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder setup <a class=\"anchor\" id=\"folders\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(directory_path + \"\\\\utils\")\n",
    "sys.path.append(directory_path + \"\\\\scripts\")\n",
    "sys.path.append(directory_path + \"\\\\notebooks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB connection string is stored in .env folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from MongoDB <a class=\"anchor\" id=\"read\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_docs import ReadDocs\n",
    "data_access = ReadDocs(os.environ.get('MONGODB_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access.list_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access.list_collections(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = data_access.read_tweets_in_collection(\"tweets\",\"global\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "Preprocessing functions to standardize the tweets one word at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base = os.path.dirname(os.getcwd())\n",
    "data_folder = project_base + r\"\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to reading from MongoDB cluster\n",
    "import pandas as pd\n",
    "tweets_df = pd.read_json(data_folder+r\"\\global_twitter_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_preprocessing import TweetsPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt i_ameztoy extra random image i\\n\\nlets focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt indopac_info #chinas media explains the mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>china even cut off communication they dont anw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putin to #xijinping  i told you my friend taiw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt chinauncensored im sorry i thought taiwan w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>rt indopac_info a good infographic of #chinas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>rt indopac_info a good infographic of #chinas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>reuters thanks #pelosi smart move</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>rt indopac_info #taiwan peoples desire for uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>rt indopac_info #taiwan peoples desire for uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               processed\n",
       "0      rt i_ameztoy extra random image i\\n\\nlets focu...\n",
       "1      rt indopac_info #chinas media explains the mil...\n",
       "2      china even cut off communication they dont anw...\n",
       "3      putin to #xijinping  i told you my friend taiw...\n",
       "4      rt chinauncensored im sorry i thought taiwan w...\n",
       "...                                                  ...\n",
       "21995  rt indopac_info a good infographic of #chinas ...\n",
       "21996  rt indopac_info a good infographic of #chinas ...\n",
       "21997                  reuters thanks #pelosi smart move\n",
       "21998  rt indopac_info #taiwan peoples desire for uni...\n",
       "21999  rt indopac_info #taiwan peoples desire for uni...\n",
       "\n",
       "[22000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_prep = TweetsPreprocessing()\n",
    "processed_df = tweets_prep.preprocess_tweets_df(tweets_df, \"full_text\", 'n')\n",
    "display(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling <a class=\"anchor\" id=\"modelling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modelling import TopicModelling\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = TopicModelling()\n",
    "tweet_mappings = topic_model.make_dictionary(processed_df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "topic_model.save_dictionary(tweet_mappings,'global_mappings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = topic_model.create_bow(processed_df, tweet_mappings)\n",
    "topic_model.serialize_bow('global_bow',bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\10academyrepositories\\week0\\Twitter_Data_Analysis_Project\\tweets\\Lib\\site-packages\\gensim\\models\\ldamodel.py:850: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    }
   ],
   "source": [
    "lda = topic_model.create_lda_model(bow, tweet_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Look into this visualization: maybe try using a single Tweet text instead of an entire dataframe list???\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "display(topic_model.visualize_lda_results(lda, bow, tweet_mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.save_lda_objects(lda, 'global_lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Try loading saved objects i.e. lda model, mappings, bag of word corpus\n",
    "\n",
    "**TODO**: Filter the tweets based on the list comprehension code that he used. Do not save formatted text as dataframe instead save as list.\n",
    "(Putting it all together)\n",
    "\n",
    "**TODO**: Pick a topic of interest to investigate (Visualization-guided analysis)\n",
    "\n",
    "**TODO**: Create a wordcloud (Visualize topic 2 with a word cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
